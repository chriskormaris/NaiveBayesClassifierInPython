# NaiveBayesClassifier

Made by Chris Kormaris

Programming Language: Python

Unzip the files "TRAIN.zip" and "TEST.zip", in the same directory where the Python files are. First, run the python file "FeatureSelectionUsingIG.py" to generate the output file "feature_dictionary.txt", containing the features tokens that we'll use.
Feature selection for the most useful tokens, using Information Gain (IG) has been implemented in thi file. The feature tokens are boolean, as in they take boolean values, 0 if the token does not appear in a text or 1 if the token appears in a text. The boolean values are assigned while generating the feature vectors of each text. At first, all the files of the corpus are being parsed and we count in how many spam or ham documents in total, each word appears. The results are being saved in "dictionary" data structures with the names: "feature_spam_frequency", "feature_ham_frequency" and "feature_frequency" respectively, with feature tokens being the keys and and frequencies being the values. These "dictionary" variables are used for the calculation of the propabilities of the Information Gain algorithm. Then, we calculate the entropy H(C) calculated and print it. The information gain for each score for each feature token is calculated using the formula: IG(X , C) = IG (C , X) = H(C) - \sum {P (X=x) \cdot H (C|X =x)}. Concretely, the feature x that reduces the entropy less is the most desired candidate feature because it can discriminate the category of a document more efficiently. The number of feature tokens the feature selection algorithm returns is set to 1000, alternatively 100. The number of feature tokens to use depends on the classification algorithm that we'll use. Since we are building a Naive-Bayes classifier we set the number of features to 100. Using less features is more likely to make better predictions if Naive-Bayes is the classifier of our choice. In addition, there has been implemented an alternative way for calculating the Information Gain score, by using the following formula: IG(X, C)_{alternative} = IG(C, X)_{alternative} = |P(X=1|C=0) - P (X=1|C=1)|, which is the absolute difference between the conditional propabilities for the 2 classes (spam or ham). The tokens for which this absolute difference is bigger are selected as feature tokens of the feature vectors, based on which we will classify our corpus files. Using the feature tokens of this formula, slightly worse accuracy has been observed. In the end of the program "FeatureSelectionWithIG.py", a dictionary will be created, which contains the feature tokens that will be used for the Naive-Bayes classifier and it is saved in a file called "feature_dictionary.txt"

After the feature selection step, run "MyCustomNaiveBayesSpamHam.py" to start the classification process.


Note: You can use your own Train and Test text files if you want, as long as they contain "spam" or "ham" in their names, according to their category. The existence of the substrings "spam" or "ham" in a text file defines in which category of the two the text file belongs to.
